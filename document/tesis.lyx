#LyX 1.4.3 created this file. For more info see http://www.lyx.org/
\lyxformat 245
\begin_document
\begin_header
\textclass book
\begin_preamble
\renewcommand\[{\begin{equation}}
\renewcommand\]{\end{equation}} 
\end_preamble
\language english
\inputencoding auto
\fontscheme default
\graphics default
\paperfontsize default
\spacing single
\papersize letterpaper
\use_geometry true
\use_amsmath 1
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\leftmargin 3cm
\topmargin 3cm
\rightmargin 3cm
\bottommargin 3cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip smallskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\end_header

\begin_body

\begin_layout Title
Paralell Training of Linear Transductive 
\begin_inset ERT
status collapsed

\begin_layout Standard

{SVM}
\end_layout

\end_inset

s for Automated Text Categorization
\end_layout

\begin_layout Author
\begin_inset ERT
status collapsed

\begin_layout Standard

DRAFT
\backslash

\backslash
 
\backslash

\backslash
 
\backslash

\backslash
 
\backslash

\backslash
 
\end_layout

\begin_layout Standard

\end_layout

\end_inset

 Miguel Fernando Cabrera Granados
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash

\backslash
mfcabrer@unal.edu.co
\end_layout

\begin_layout Standard

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash

\backslash
 
\backslash

\backslash
 
\backslash

\backslash
 
\backslash

\backslash
 
\backslash

\backslash
 
\backslash

\backslash

\end_layout

\begin_layout Standard

Advisor
\backslash

\backslash

\end_layout

\begin_layout Standard

Jairo Jos
\backslash
'{e} Espinosa  
\backslash

\backslash
 
\backslash

\backslash
 
\backslash

\backslash
 
\backslash

\backslash
 
\backslash

\backslash

\end_layout

\begin_layout Standard

Presented in Partial Fulfillment 
\backslash

\backslash
 
\end_layout

\begin_layout Standard

of the Requirements 
\backslash

\backslash

\end_layout

\begin_layout Standard

for the Degree of 
\backslash

\backslash

\end_layout

\begin_layout Standard

Ingeniero de Sistemas e Inform
\backslash
'{a}tica 
\backslash

\backslash
 
\backslash

\backslash
 
\backslash

\backslash
 
\end_layout

\begin_layout Standard

Escuela de Sistemas - Facultad de Minas
\backslash

\backslash

\end_layout

\begin_layout Standard

Universidad Nacional de Colombia - Sede Medell
\backslash
'{i}n
\backslash

\backslash

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset LatexCommand \tableofcontents{}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Standard
GENERAL NOTES:
\end_layout

\begin_layout Standard
* When in Transductive take the example of web un-categorized data.
\end_layout

\begin_layout Standard
Find application of text categorization (Joachims Mentions many)
\end_layout

\begin_layout Standard
* In Information retrieval justify the usage of automatic techiques using
 the citation on the CMU Phd Thesis.
\end_layout

\begin_layout Standard
* Justify the usage of SVM for text classification for the reason exhibited
 in Sebastini's survey
\end_layout

\begin_layout Standard
* Justify the paralellization because collobert paper about generalization
 improvement through paralellization (CITE THEM)
\end_layout

\begin_layout Standard
* The Thesis of This girl check out the part of Text Representation and
 Empirical Risk Minization Principle
\end_layout

\begin_layout Standard
* Say in Paralell SVM that the parallelization of is difficult cause the
 dependency of the steps altoguh there have been approach to deal with this
 problem (Zanni paper)
\end_layout

\begin_layout Standard
* Change the parallell part of a review fof the tendency of parallell multicore
 processors and HPC systems.
 and basic parallelization vector and paraellization
\end_layout

\begin_layout Standard
* Usage of MapReduce to Parallelize ML Algorithm
\end_layout

\begin_layout Standard
* Complete IR part with thesis of the guy from CMU, Austrlia and the girla
 from IDIAP.
\end_layout

\begin_layout Standard
* Potter Stemmer reference
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Overview
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Text classification is a key aspect of text filtering, document management
 and retrieval tasks.
 Besides basic document classification for somekind of digital library,
 many problems can be seen as instances of the Text Categorization (TC)
 problem .
 Spam detection, Web search improvement and automated metadata generation
 are just a few examples of this
\begin_inset Note Note
status collapsed

\begin_layout Standard
of the tasks that can be accomplished using automatic techniques for document
 classification
\end_layout

\end_inset

 
\begin_inset LatexCommand \cite{Sebastiani02}

\end_inset

.
 
\begin_inset Note Note
status collapsed

\begin_layout Standard
Add here the fact that the manual classification is at best expensive or
 impossible.
 Also add more examples of automated categorization usages
\end_layout

\end_inset

 Some of those tasks can be achieved by human
\emph on
 
\emph default
beings, but a manual classification is at best expensive and practically
 impossible for large amounts of documents found today in modern information
 systems.
\end_layout

\begin_layout Standard
A TC technique uses example documents that have been previously categorized
 in classes by an authority in order to learn a model that, with an associated
 error value, can automatically predict the class that the authority would
 have given to future documents.
 
\end_layout

\begin_layout Standard
Support Vector Machines 
\begin_inset LatexCommand \cite{Vapnik98}

\end_inset

 are a powerful tool for classifying large datasets, and due the nature
 of the classical text representation models, it has been applied successfully
 in automated document classification tasks 
\begin_inset LatexCommand \cite{Joachims98}

\end_inset


\begin_inset LatexCommand \cite{Joachims99c}

\end_inset

.
 An special type of SVM, based on transductive inference (TSVM), has demonstrate
d to be more effective for document classification than the common inductive
 inference based SVMs 
\begin_inset LatexCommand \cite{Joachims99c}

\end_inset

.
\end_layout

\begin_layout Standard
One characteristic of the SVM is that, in its formal definition, the computation
 and storage requirements increase rapidly with the number of training vectors.
 This is due the fact that the SVM classification problem is a Quadratic
 Programming problem (QP) that finds the support vectors in all training
 dataset.
 Solvers of this kind of problem generally scales to 
\begin_inset Formula $O(n^{3})$
\end_inset

 making it a very computationally expensive problem.
\end_layout

\begin_layout Standard
One approach to cope with this limitation is to divide the problem into
 chunks 
\begin_inset LatexCommand \cite{Joachims/99a}

\end_inset


\begin_inset LatexCommand \cite{osunaetal97}

\end_inset

 and train those subproblems.
 Even with these optimizations, the problem still has large computational
 requirements, and the required time to train grows sufficiently enough
 for making it not useful for real-time training when using large datasets.
 The implementation of Transductive inference for a SVM requires to solve
 the same problem many times over generally large datasets, until finding
 the optimal classifier.
 This makes the scaling problem for transductive SVM even more difficult,
 therefore, methods for optimizing the training process have to be developed.
\end_layout

\begin_layout Standard
Taking advantage actual trends in processor technologies, where the multi-core
 processor is becoming the norm[CitationHere] 
\begin_inset LatexCommand \cite{Marowka07}

\end_inset

 parallel implementation of such algorithm along with other optimization
 will help make the application of this technique practical in real world
 situations.
 Also, as an extra motivation, empirical results have showed that some parallel
 settings for SVM have achieved better generalization than their classic
 counterparts 
\begin_inset LatexCommand \cite{citeulike:935557|Collobert2002}

\end_inset

.
\end_layout

\begin_layout Standard
This work describes an implementation of a parallel SVM using the cascade
 model described in 
\begin_inset LatexCommand \cite{GrafCBDV04}

\end_inset

 using a transductive learning algortithm.
 The first part, Overview introduces the basic concepts a Machine Learning,
 Information Retrieval and Text Categorization.
 Later we describe the SVM algorithm and justify the reason of using SVM
 for text classification over other techniques.
 In chapter 
\begin_inset LatexCommand \ref{cha:Experiments-and-Results}

\end_inset

 we exhibit our experimental set-up and the results.
\end_layout

\begin_layout Section
Machine Learning
\end_layout

\begin_layout Standard
Machine Learning (
\noun on
ML
\noun default
) is concerned with the question of how to construct computer programs that
 automatically improve with experience 
\begin_inset LatexCommand \cite{Mitchell97}

\end_inset

.
 The experience encounter by the computer program are examples that it takes
 as input.
 In order to to develop such programs 
\noun on
ML
\noun default
 borrows concepts from many areas suchs Statistics, Information Theory,
 Biology and Control Theory, futhermore, given the amount of statistical
 based ML algorithms developed in the last years, many scientists see ML
 as a crossroad between Statistics and Computer Science.
 The learning problem is defined formally in 
\begin_inset LatexCommand \cite{Mitchell97}

\end_inset

 as follows:
\end_layout

\begin_layout Description
Learning: A computer program is said to 
\emph on
learn 
\emph default
from experience 
\begin_inset Formula $E$
\end_inset

 with respect to some class of Task 
\begin_inset Formula $T$
\end_inset

 and performace measuer 
\begin_inset Formula $P$
\end_inset

, if its performace at tasks in 
\begin_inset Formula $T$
\end_inset

, as measured by 
\begin_inset Formula $P$
\end_inset

 improves with 
\begin_inset Formula $E$
\end_inset

.
 
\begin_inset Note Note
status collapsed

\begin_layout Standard

\end_layout

\end_inset


\end_layout

\begin_layout Standard
In other words, the program learns from an experience commonly represented
 by a group of data belonging to a universe on which the result of Task
 
\begin_inset Formula $T$
\end_inset

 is known.
 Using this information the computer program tries to generalize for over
 the available data, thus learning a representation of the universe.
 
\end_layout

\begin_layout Standard
The way in which an actual computer program realizes the task of learning
 from the experience can different, there are serveral modes in which the
 learning task can be accomplished 
\begin_inset LatexCommand \cite{citeulike:755348|Heibrich2001Lkernel}

\end_inset

:
\end_layout

\begin_layout Itemize

\emph on
Supervised Learning
\emph default
: In wich the program generates an internal representation that maps inputs,
 to desired ouputs.
 Classification problems can be seen as an instance of this type of learning,
 where the desired output is a binary value stating wheter the input belongs
 to a specific category.
\end_layout

\begin_layout Itemize

\emph on
Unsupervised Learning: 
\emph default
Combines both labeled and unlabeled examples to generate the representation.
\end_layout

\begin_layout Itemize

\emph on
Reinforcement Learning
\emph default
: In wich the algorithm learns a policy of how to act given an observation
 of the world.
\end_layout

\begin_layout Standard
Besides these three main ways of learning, there are others that can be
 seen as variations of the ones mentioned above.
\end_layout

\begin_layout Itemize

\emph on
Semi-supevised Learning
\emph default
: Combines both labeled and unlabeled examples to generate the representation.
\end_layout

\begin_layout Itemize

\emph on
Transductive Learning
\emph default
: Similar to Semi-supervised Learning, but does not explicitly construct
 a function: instead, tries to predict new outputs based on training inputs,
 training outputs and test input wich are available while training.

\emph on
 
\end_layout

\begin_layout Standard
In this thesis we are going to use a restricted definition in order to ilustrate
 the concepts described above.
 In these pages machine learning will refer to a generalized regression
 characterizing a set of label events 
\begin_inset Formula $\left\{ (x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\right\} $
\end_inset

 with a function 
\begin_inset Formula $\Phi:X\rightarrow Y$
\end_inset

 from event to label .
 Many researcher has used this setting with success 
\begin_inset LatexCommand \cite{Berger2001}

\end_inset

 .
 The reader will notice the similarity between this definition and the definitio
n of text categorization described in section 
\begin_inset LatexCommand \vref{sec:Text-Categorization}

\end_inset

.
 
\end_layout

\begin_layout Standard
The question of how good the function 
\begin_inset Formula $\Phi$
\end_inset

 can generalize has spawned an entire subfield of machine learning called
 computational learning theory.
 Many of the techniques classified in this field have come from development
 of frameworks in this particular subfield.
 These frameworks generally don't state how good is going to perform an
 algorithm, instead create a probabilistic bound to the performace.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Standard
characterizing a set of labeled events f(x1; y1); (x2; y2); : : : (xn; yn)g
 with a function : X ! Y from event to label (or 
\backslash
output").
\end_layout

\end_inset

 Some of techniques generally classified under ML are Artificial Neural
 Networks, Genetics Algorithm, k-Nearest Neighbor, Bayesian Networks and
 and Support Vector Machines (SVM) and has been succesfully applied to.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Standard
* overview
\end_layout

\begin_layout Standard
* Learning taxonomy
\end_layout

\begin_layout Standard
Machine learning algorithms are organized into a taxonomy, based on the
 desired outcome of the algorithm.
 Common algorithm types include:
\end_layout

\begin_layout Standard
* Supervised learning ? in which the algorithm generates a function that
 maps inputs to desired outputs.
 One standard formulation of the supervised learning task is the classification
 problem: the learner is required to learn (to approximate) the behavior
 of a function which maps a vector [X_1, X_2, 
\backslash
ldots X_N]
\backslash
, into one of several classes by looking at several input-output examples
 of the function.
 * Unsupervised learning ? which models a set of inputs: labeled examples
 are not available.
 * Semi-supervised learning ? which combines both labeled and unlabeled
 examples to generate an appropriate function or classifier.
 * Reinforcement learning ? in which the algorithm learns a policy of how
 to act given an observation of the world.
 Every action has some impact in the environment, and the environment provides
 feedback that guides the learning algorithm.
 * Transduction ? similar to supervised learning, but does not explicitly
 construct a function: instead, tries to predict new outputs based on training
 inputs, training outputs, and test inputs which are available while training.
 * Learning to learn ? in which the algorithm learns its own inductive bias
 based on previous experience.
\end_layout

\begin_layout Standard
The computational analysis of machine learning algorithms and their performance
 is a branch of theoretical computer science known as computational learning
 theory.
\end_layout

\begin_layout Standard
* differences 
\end_layout

\begin_layout Standard
* statistical machine learning
\end_layout

\begin_layout Standard
* the natural application of machine learning to information retrieval
\end_layout

\end_inset


\end_layout

\begin_layout Section
Information Retrieval
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Standard
Information Retrieval (IR) is branch of computer science whose originally
 objective was to obtain information, only text in early stages, from generally
 large amounts of data.
 Information Retrieval (IR) is very big term that comprises a lot of concepts
 regarding data manipulation, where data can be anything from text to videos.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Information Retrieval (IR) is branch of computer science whose original
 objective was to obtain information from generally large amounts of data.
 Today modern IR deals with content-based document managment task, including
 document filtering, translation, summarization, question answering among
 others.
 These last tasks can be seen as "knowledge" retrieval instead of pure 
\begin_inset Quotes eld
\end_inset

information
\begin_inset Quotes erd
\end_inset

, therefore the application of Artificial Intelligence (AI), specifically
 Machine Learning to improve or to help automatize these tasks is natural.
 The research that involves AI techniques for improving IR systems is known
 as Intelligent Information Systems.
\end_layout

\begin_layout Standard
Altough we could generalize document to be any kind of data such as video,
 music and images, when I refer to IR in this document I will will be refereing
 to text based documents.
\end_layout

\begin_layout Standard
IR Systems makes large amounts of text accessible to people that need the
 information, A user the apporaches to the system with a vague idea of what
 is looking for, so the goal of the system is to provide the information
 required.
 The user provides generally provides some information about what kind of
 documents is looking for, in the form of keywords or, in systems with a
 categorical arrangement, 
\emph on
browse through a 
\emph default
the tree of categories.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Standard
Information storage and retrieval systems make large volumes of text accessible
 to people with information needs (van Rijsbergen, 1979; Salton and McGill,
 1983).
 A person approaches such a system with some idea of what they want to find
 out, and the goal of the system is to fulfill that need.
 The person (hereafter referred to as the user) provides an outline of their
 requirements?perhaps a list of keywords relating to the topic in question,
 or even an example document.
 The system searches its database for documents that are related to the
 user?s query, and presents those that are most relevant.
 General texts relating to information retrieval and computer implementation
 of retrieval systems are Salton (1988), Frakes (1992a), Baeza-Yates (1992),
 and Witten et al.
 (1994).
\end_layout

\end_inset

 
\end_layout

\begin_layout Subsection
Models of Text and Text Representation
\end_layout

\begin_layout Subsection
Dimensionality Reduction
\end_layout

\begin_layout Section
Text Categorization
\begin_inset LatexCommand \label{sec:Text-Categorization}

\end_inset


\end_layout

\begin_layout Standard
The goal of text categorization is to automatically assign, for each documents,
 categories selected among a predefined set.
 In opposition to the machine learning typical multi-class classification
 task which aims at attributing one class among the possible ones to each
 example, documents in a text categorization task may belong to several
 categories 
\end_layout

\begin_layout Subsection
Applications of Text Categorization
\end_layout

\begin_layout Subsection
Taxonomy of Text Categorization Techniques
\end_layout

\begin_layout Subsection
Techniques
\end_layout

\begin_layout Section
Paralell Machine Learning
\end_layout

\begin_layout Subsection
Tools
\end_layout

\begin_layout Chapter
Text Categorization With SVM
\end_layout

\begin_layout Section
Support Vector Machines
\end_layout

\begin_layout Standard
Support Vector Machines (SVMs) 
\begin_inset LatexCommand \cite{Vapnik98}

\end_inset

 are powerful classification and regresion tools that have been widely applied
 in the solutions of many problem, generally yielding comparable or even
 better performance that other algortihms.
 
\end_layout

\begin_layout Standard
The SVM algorithm is based on the concept of VC Dimension
\begin_inset LatexCommand \cite{vapnik71uniform}

\end_inset

 and on the principle of empirical risk minimization developed by Vapnik
 
\begin_inset LatexCommand \cite{Vapnik98}

\end_inset


\begin_inset LatexCommand \cite{Vapnik99}

\end_inset

.
 Roughly, the VC dimension is a metric of how complex a classifier machine
 is, complex classifier has more capacity to fit the training data, thus,
 overfitting is more likely to occur, therefore, is prefered a classifier
 with minimum VC Dimension.
 
\begin_inset Note Note
status collapsed

\begin_layout Standard
Shoud I add the formula of Empirical Risk Minimization
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The basic idea of empirical risk minimization principle is to find an hypothesis
 
\begin_inset Formula $s$
\end_inset

 from an hypothesis space 
\begin_inset Formula $S$
\end_inset

 for which the lowest probability of error is guaranteed for a given set
 of training examples.
 
\end_layout

\begin_layout Standard
For linear classification problems this is equivalent to find the discrimination
 function that maximizes the distances within the classes, assuring the
 lowest probability of error
\begin_inset LatexCommand \cite{citeulike:368926|Haykin1998}

\end_inset

.
 The aim of the Support Vector classification is to devise a computationally
 efficient way of learning the optimal separating hyperplanes in high dimensiona
l feature space 
\begin_inset LatexCommand \cite{citeulike:114719|Cristianini2000introSVM}

\end_inset

, 
\begin_inset Note Note
status collapsed

\begin_layout Standard
efining optimal hyperplane as the one that has both, the minimum empirical
 risk and VC dimension.
 .
 
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Finding the Optimal Hyperplane
\end_layout

\begin_layout Standard
Let us consider a training sample 
\begin_inset Formula $\{(x_{i},d_{i})\}_{i=1}^{N}$
\end_inset

, where x is a point in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 that belongs to the group of training examples or input patterns for 
\begin_inset Formula $i$
\end_inset

th example and 
\begin_inset Formula $d_{i}$
\end_inset

 is the corresponded, desired response.
 In this case we are gonna simplifly the setting by stating that 
\begin_inset Formula $d_{i}$
\end_inset

 can only take two values 
\begin_inset Formula $\{+1,-1\}$
\end_inset

 for the positive and negative classification case respectively.
 This is case of a binary classification problem, for multiclass classification
 problem as described back in [INT-REF HERE] we can transform the problem
 in different binary classification ones.
 We also assume that the class represented by 
\begin_inset Formula $d_{i}=\{+1,-1\}$
\end_inset

 are lineary separable.
 The equation of a decision hyperplane that separates the data is:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
w^{T}x+b=0\label{eq:}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Standard
\align center
\begin_inset Graphics
	filename images/svms-1-basic.jpg
	scale 50

\end_inset


\end_layout

\begin_layout Caption
The circles are the training data, the filled ones for 
\begin_inset Formula $d_{i}=+1$
\end_inset

 class and the empty ones for the 
\begin_inset Formula $d_{i}=-1$
\end_inset

 class.
 The points on 
\begin_inset Formula $H1$
\end_inset

 and 
\begin_inset Formula $H2$
\end_inset

 are the support vector that maximize the distance between the data and
 the hyperplane defined by 
\begin_inset Formula $w$
\end_inset

, a normal vector to the hyperplane and the offset 
\begin_inset Formula $-b/|w|$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $x$
\end_inset

 is an input vector, 
\begin_inset Formula $w$
\end_inset

 is the weight vector, normal to the separation hyperplane, and 
\begin_inset Formula $b$
\end_inset

 is the offset or bias:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
 & w^{T}x+b\geq0\mbox{\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,}for\,\,\,\,\,\,\, d_{i}=+1\\
 & w^{T}x+b<0\mbox{\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,}for\,\,\,\,\,\,\, d_{i}=-1\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
For a given vector 
\begin_inset Formula $w$
\end_inset

 and offset 
\begin_inset Formula $b$
\end_inset

 the distance between the the hyperplane defined in and a point 
\begin_inset Formula $x$
\end_inset

 is called margin of separation represented by the equation
\begin_inset LatexCommand \ref{eq:}

\end_inset

.
\end_layout

\begin_layout Standard
Then we can write the discriminat function:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
{g(x)=w}^{T}x+b\label{eq:2}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
An the classification function as:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
sing\{ g(x)=w^{T}x+b\}\label{eq:3}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Equation 
\begin_inset LatexCommand \ref{eq:2}

\end_inset

 gives a measure of the distance from x to the optimal hyperplane, and 
\begin_inset LatexCommand \ref{eq:3}

\end_inset

 assing a category for a specific point.
 
\end_layout

\begin_layout Standard
As described above, the complexity of the classifier should be minimized
 in order to achieve a good generalization, for linear classifier this equivalen
t lowering that complexity is to maximize the margin of separation between
 the points and the hyperplane.
 An approach to find this separator is to maximize the margin between two
 parallel supporting planes, if we define the margin as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
 & w^{T}x+b=\,\,\,1\\
 & w^{T}x+b=-1\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Then the margin of separation is equal then to 
\begin_inset Formula $\rho=\frac{2}{w}$
\end_inset

 .
 Figure [TALES#1] shows this approach.
 Now our problem becomes maximize 
\begin_inset Formula $\frac{2}{w}$
\end_inset

 which is equivalent to minimize 
\begin_inset Formula $||w||_{2}/2$
\end_inset

 in the following quadratic programming problem:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray}
 & \underset{w,b}{min} & \frac{1}{2}||w||^{2}\nonumber \\
s.t &  & d_{i}(w^{T}x_{i}+b)\geq1\label{eq:min0.5w2}\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
Condition 
\begin_inset Formula $d_{i}(w^{T}x_{i}+b)\geq1$
\end_inset

 spcify that all the data points have to be classified correctly.
 In order to solve this problem we introduce the lagrangian formulation:
\end_layout

\begin_layout Standard
\SpecialChar \-

\begin_inset Formula \begin{eqnarray}
\,\underset{\alpha}{max}\,\underset{w,b}{min}\,\, J(w,b,\alpha) & = & \frac{1}{2}w^{T}w-\sum_{i=1}^{N}\alpha_{i}[d_{i}(w^{T}x_{i}+b)-1]\label{eq:lagrangian}\\
s.t &  & \alpha_{i}\geq0\nonumber \end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
Now the conditions of optimality for 
\begin_inset Formula $J(w,b,\alpha)$
\end_inset

 are given by:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\,\,\,\,\,\frac{\delta J(w,b,\alpha)}{\delta w}=0\longrightarrow w=\sum_{i=1}^{N}\alpha_{i}d_{i}x_{i}\label{eq:weqsumadx}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\frac{\delta J(w,b,\alpha)}{\delta b}=0\longrightarrow\sum_{i=1}^{N}\alpha_{i}d_{i}=0\label{eq:sumadeq0}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Replacing 
\begin_inset LatexCommand \ref{eq:weqsumadx}

\end_inset

 and 
\begin_inset LatexCommand \ref{eq:sumadeq0}

\end_inset

 into 
\begin_inset LatexCommand \ref{eq:lagrangian}

\end_inset

 and defining the cost function as 
\begin_inset Formula $J(w,b,\alpha)=Q(\alpha)$
\end_inset

 yields:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray}
min\, Q(\alpha) & = & \sum_{i=1}^{N}\alpha-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}d_{i}d_{j}x_{i}^{T}x_{j}\label{eq:minq}\\
s.t &  & \alpha\geq0\,\, and\,\,\,\sum_{i=1}^{N}\alpha_{i}d_{i}=0\nonumber \end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
Which is a quadratic programming problem (QP).
 Notice that the dual problem is cast entirely in terms of the training
 data.
 The fact that this is a QP problem is one of the streght of this methodology
 and also a weakness.
 It's a streght because as 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
opposed to others techniques such as Neural Networks, the optimization problem
 is convex and a global optimum is always reached in a finite amount of
 time.[WHERE IN HELL CAN I CITE THIS???'] On the other hand, a QP is a computatio
nally expensive problem, this property has led to the development of both,
 novel ways of solving the problem described in equation 
\begin_inset LatexCommand \ref{eq:minq}

\end_inset

 and alternative formulations for solving the same optimization problem
 
\begin_inset LatexCommand \cite{Joachims/06a}

\end_inset

 , with variying degrees of performace.
\end_layout

\begin_layout Section
SVM and Text Categorization
\end_layout

\begin_layout Standard
The usage of SVM was first introduced by Joachims 
\begin_inset LatexCommand \cite{Joachims98}

\end_inset


\begin_inset LatexCommand \cite{Joachims99c}

\end_inset

 and similar setups have been used in posterior literature[CITATIONS HERE!]
 .
 the TC task using SVM can bee seen geometrically as finding an hyperplane
 that separates two grous of points has 
\end_layout

\begin_layout Subsection
Linear SVM for Text Categorization
\end_layout

\begin_layout Subsection
Transductive Learning for SVM
\end_layout

\begin_layout Subsection
Linear Transductive Learning SVM for Text Categorization
\end_layout

\begin_layout Section
Paralell Method for Training Transuductive SVM
\end_layout

\begin_layout Subsection
Transductive Parallel Linear SVM
\end_layout

\begin_layout Chapter
Experiments and Results
\begin_inset LatexCommand \label{cha:Experiments-and-Results}

\end_inset


\end_layout

\begin_layout Section
Experimental Setting
\end_layout

\begin_layout Subsection
Software
\end_layout

\begin_layout Subsection
Data Sets
\end_layout

\begin_layout Subsection
The Cascade SVM
\end_layout

\begin_layout Standard
For the experiment we use cascade SVM 
\begin_inset LatexCommand \cite{GrafCBDV04}

\end_inset

 
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Section
Conclusions
\end_layout

\begin_layout Standard
\begin_inset LatexCommand \bibtex[acm]{bibblio}

\end_inset


\end_layout

\end_body
\end_document
