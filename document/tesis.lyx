#LyX 1.5.1 created this file. For more info see http://www.lyx.org/
\lyxformat 276
\begin_document
\begin_header
\textclass book
\begin_preamble
\renewcommand\[{\begin{equation}}
\renewcommand\]{\end{equation}} 
\end_preamble
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\paperfontsize default
\spacing single
\papersize letterpaper
\use_geometry true
\use_amsmath 1
\use_esint 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\leftmargin 3cm
\topmargin 5cm
\rightmargin 3cm
\bottommargin 3cm
\headheight 5cm
\headsep 1cm
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\defskip smallskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle headings
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Paralell Training of Linear Transductive 
\begin_inset ERT
status collapsed

\begin_layout Standard

{SVM}
\end_layout

\end_inset

s for Automated Text Categorization
\end_layout

\begin_layout Author
\begin_inset ERT
status collapsed

\begin_layout Standard

DRAFT
\backslash

\backslash
 
\backslash

\backslash
 
\backslash

\backslash
 
\backslash

\backslash
 
\end_layout

\begin_layout Standard

\end_layout

\end_inset

 Miguel Fernando Cabrera Granados
\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash

\backslash
mfcabrer@unal.edu.co
\end_layout

\begin_layout Standard

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Standard


\backslash

\backslash
 
\backslash

\backslash
 
\backslash

\backslash
 
\backslash

\backslash
 
\backslash

\backslash
 
\backslash

\backslash

\end_layout

\begin_layout Standard

Advisor
\backslash

\backslash

\end_layout

\begin_layout Standard

Jairo Jos
\backslash
'{e} Espinosa  
\backslash

\backslash
 
\backslash

\backslash
 
\backslash

\backslash
 
\backslash

\backslash
 
\backslash

\backslash

\end_layout

\begin_layout Standard

Presented in Partial Fulfillment 
\backslash

\backslash
 
\end_layout

\begin_layout Standard

of the Requirements 
\backslash

\backslash

\end_layout

\begin_layout Standard

for the Degree of 
\backslash

\backslash

\end_layout

\begin_layout Standard

Ingeniero de Sistemas e Inform
\backslash
'{a}tica 
\backslash

\backslash
 
\backslash

\backslash
 
\backslash

\backslash
 
\end_layout

\begin_layout Standard

Escuela de Sistemas - Facultad de Minas
\backslash

\backslash

\end_layout

\begin_layout Standard

Universidad Nacional de Colombia - Sede Medell
\backslash
'{i}n
\backslash

\backslash

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Standard
POR HACER:
\end_layout

\begin_layout Standard
- Abstract and Resume
\end_layout

\begin_layout Standard
- Transductive Theory
\end_layout

\begin_layout Standard
- Text Classification Theory
\end_layout

\begin_layout Standard
- Experment setup
\end_layout

\begin_layout Standard
- Text Categorization with SVM
\end_layout

\begin_layout Standard
GENERAL NOTES:
\end_layout

\begin_layout Standard
* When in Transductive take the example of web un-categorized data.
\end_layout

\begin_layout Standard
Find application of text categorization (Joachims Mentions many)
\end_layout

\begin_layout Standard
* In Information retrieval justify the usage of automatic techiques using
 the citation on the CMU Phd Thesis.
\end_layout

\begin_layout Standard
* Justify the usage of SVM for text classification for the reason exhibited
 in Sebastini's survey and Joachims Papers, and The other Reference of That
 Dumais et Al.
 and the other bechmark
\end_layout

\begin_layout Standard
* Justify the paralellization because collobert paper about generalization
 improvement through paralellization (CITE THEM)
\end_layout

\begin_layout Standard
* The Thesis of This girl check out the part of Text Representation and
 Empirical Risk Minization Principle
\end_layout

\begin_layout Standard
* Say in Paralell SVM that the parallelization of is difficult cause the
 dependency of the steps altoguh there have been approach to deal with this
 problem (Zanni paper)
\end_layout

\begin_layout Standard
* Change the parallell part of a review fof the tendency of parallell multicore
 processors and HPC systems.
 and basic parallelization vector and paraellization
\end_layout

\begin_layout Standard
* Usage of MapReduce to Parallelize ML Algorithm
\end_layout

\begin_layout Standard
* Complete IR part with thesis of the guy from CMU, Austrlia and the girla
 from IDIAP.
\end_layout

\begin_layout Standard
* Potter Stemmer reference
\end_layout

\begin_layout Standard
* Usar el paper de Magnus para justificar el uso de BOW sobre otras maneras
 de representaci√≥n de Textos
\end_layout

\begin_layout Standard
* Cite Salton Book Introduction to information retrieval and the Book of
 Baeza-Yates
\end_layout

\begin_layout Standard
* Cite the need of parallelization as multiclass problem becomes a 2*n problems
 of binary classification
\end_layout

\begin_layout Standard
* Representation in ONe Class SVM for text categorization and teh perfomance
 en Both Dumais and This guy
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Overview
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Text classification is a key aspect of text filtering, document management
 and retrieval tasks.
 Besides basic document classification for somekind of digital library,
 many problems can be seen as instances of the Text Categorization (TC)
 problem .
 Spam detection, Web search improvement and automated metadata generation
 are just a few examples of this
\begin_inset Note Note
status collapsed

\begin_layout Standard
of the tasks that can be accomplished using automatic techniques for document
 classification
\end_layout

\end_inset

 
\begin_inset LatexCommand cite
key "Sebastiani02"

\end_inset

.
 
\begin_inset Note Note
status collapsed

\begin_layout Standard
Add here the fact that the manual classification is at best expensive or
 impossible.
 Also add more examples of automated categorization usages
\end_layout

\end_inset

 Some of those tasks can be achieved by human beings, but a manual classificatio
n is at best expensive and practically impossible for large amounts of documents
 found today in modern information systems.
\end_layout

\begin_layout Standard
A TC technique uses example documents that have been previously categorized
 in classes by an authority in order to learn a model that, with an associated
 error value, can automatically predict the class that the authority would
 have given to future documents.
 
\end_layout

\begin_layout Standard
Support Vector Machines 
\begin_inset LatexCommand cite
key "Vapnik98"

\end_inset

 are a powerful tool for classifying large datasets, and due the nature
 of the classical text representation models, it has been applied successfully
 in automated document classification tasks 
\begin_inset LatexCommand cite
key "Joachims98,Joachims99c"

\end_inset

.
 An special type of SVM, based on transductive inference (TSVM), has demonstrate
d to be more effective for document classification than the common inductive
 inference based SVMs 
\begin_inset LatexCommand cite
key "Joachims99c"

\end_inset

.
\end_layout

\begin_layout Standard
One characteristic of the SVM is that, in its formal definition, the computation
 and storage requirements increase rapidly with the number of training vectors.
 This is due the fact that the SVM classification problem is a Quadratic
 Programming problem (QP) that finds the support vectors in all training
 dataset.
 Solvers of this kind of problem generally scales to 
\begin_inset Formula $O(n^{3})$
\end_inset

 making it a very computationally expensive problem.
\end_layout

\begin_layout Standard
One approach to cope with this limitation is to divide the problem into
 chunks 
\begin_inset LatexCommand cite
key "Joachims/99a,osunaetal97"

\end_inset

 and train those subproblems.
 Even with these optimizations, the problem still has large computational
 requirements, and the required time to train grows sufficiently enough
 for making it not useful for real-time training when using large datasets.
 The implementation of Transductive inference for a SVM requires to solve
 the same problem many times over generally large datasets, until finding
 the optimal classifier.
 This makes the scaling problem for transductive SVM even more difficult,
 therefore, methods for optimizing the training process have to be developed.
\end_layout

\begin_layout Standard
Taking advantage actual trends in processor technologies, where the multi-core
 processor is becoming the norm 
\begin_inset LatexCommand cite
key "Marowka07,1069628|Geer05"

\end_inset

 parallel implementation of such algorithm along with other optimization
 will help make the application of this technique practical in real world
 situations.
 Also, as an extra motivation, empirical results have showed that some parallel
 settings for SVM have achieved better generalization than their classic
 counterparts for some problems 
\begin_inset LatexCommand cite
key "citeulike:935557|Collobert2002"

\end_inset

.
\end_layout

\begin_layout Standard
This work describes an implementation of a parallel SVM using the cascade
 model described in 
\begin_inset LatexCommand cite
key "GrafCBDV04"

\end_inset

 using a transductive learning algortithm.
 The first part, Overview, introduces the basic concepts a Machine Learning,
 Information Retrieval and Text Categorization.
 Later we describe the SVM algorithm and justify the reason of using SVM
 for text classification over other techniques.
 In chapter 
\begin_inset LatexCommand ref
reference "cha:Experiments-and-Results"

\end_inset

 we exhibit our experimental set-up and the show the analisys of the results.
\end_layout

\begin_layout Section
Machine Learning
\end_layout

\begin_layout Standard
Machine Learning (
\noun on
ML
\noun default
) is concerned with the question of how to construct computer programs that
 automatically improve with experience 
\begin_inset LatexCommand cite
key "Mitchell97"

\end_inset

.
 The experience encountered by the computer program are examples that it
 takes as input.
 In order to to develop such programs 
\noun on
ML
\noun default
 borrows concepts from many areas suchs Statistics, Information Theory,
 Biology and Control Theory, futhermore, given the amount of statistical
 based ML algorithms developed in the last years, many scientists see ML
 as a crossroad between Statistics and Computer Science.
\end_layout

\begin_layout Standard
The learning problem is defined formally in 
\begin_inset LatexCommand cite
key "Mitchell97"

\end_inset

 as follows:
\end_layout

\begin_layout Description
Learning: A computer program is said to 
\emph on
learn
\emph default
 from experience 
\begin_inset Formula $E$
\end_inset

 with respect to some class of Task 
\begin_inset Formula $T$
\end_inset

 and performace measuer 
\begin_inset Formula $P$
\end_inset

, if its performace at tasks in 
\begin_inset Formula $T$
\end_inset

, as measured by 
\begin_inset Formula $P$
\end_inset

 improves with 
\begin_inset Formula $E$
\end_inset

.
 
\begin_inset Note Note
status collapsed

\begin_layout Standard

\end_layout

\end_inset


\end_layout

\begin_layout Standard
In other words, the program learns from an experience commonly represented
 by a group of data belonging to a universe on which the result of Task
 
\begin_inset Formula $T$
\end_inset

 is known.
 Using this information the computer program tries to generalize for over
 the available data, thus learning a representation of the universe.
 
\end_layout

\begin_layout Standard
The way in which an actual computer program realizes the task of learning
 from the experience can different, there are serveral modes in which the
 learning task can be accomplished 
\begin_inset LatexCommand cite
key "citeulike:755348|Heibrich2001Lkernel"

\end_inset

:
\end_layout

\begin_layout Itemize

\emph on
Supervised Learning
\emph default
: In wich the program generates an internal representation that maps inputs,
 to desired ouputs.
 Classification problems can be seen as an instance of this type of learning,
 where the desired output is a binary value stating wheter the input belongs
 to a specific category.
\end_layout

\begin_layout Itemize

\emph on
Unsupervised Learning:
\emph default
 Combines both labeled and unlabeled examples to generate the representation.
\end_layout

\begin_layout Itemize

\emph on
Reinforcement Learning
\emph default
: In wich the algorithm learns a policy of how to act given an observation
 of the world.
\end_layout

\begin_layout Standard
Besides these three main ways of learning, there are others that can be
 seen as variations of the ones mentioned above.
\end_layout

\begin_layout Itemize

\emph on
Semi-supevised Learning
\emph default
: Combines both labeled and unlabeled examples to generate the representation.
\end_layout

\begin_layout Itemize

\emph on
Transductive Learning
\emph default
: Similar to Semi-supervised Learning, but does not explicitly construct
 a function: instead, tries to predict new outputs based on training inputs,
 training outputs and test input wich are available while training.
 
\end_layout

\begin_layout Standard
In this thesis we are going to use a restricted definition in order to ilustrate
 the concepts described above.
 In these pages machine learning will refer to a generalized regression
 characterizing a set of label events 
\begin_inset Formula $\left\{ (x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\right\} $
\end_inset

 with a function 
\begin_inset Formula $\Phi:X\rightarrow Y$
\end_inset

 from event to label .
 Many researcher has used this setting with success 
\begin_inset LatexCommand cite
key "Berger2001"

\end_inset

 .
 The reader will notice the similarity between this definition and the definitio
n of text categorization described in section 
\begin_inset LatexCommand vref
reference "sec:Text-Categorization"

\end_inset

.
 
\end_layout

\begin_layout Standard
The question of how good the function 
\begin_inset Formula $\Phi$
\end_inset

 can generalize has spawned an entire subfield of machine learning called
 computational learning theory.
 Many of the techniques classified in this field have come from development
 of frameworks in this particular subfield.
 These frameworks generally don't state how good is going to perform an
 algorithm, instead create a probabilistic bound to the performace.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Standard
characterizing a set of labeled events f(x1; y1); (x2; y2); : : : (xn; yn)g
 with a function : X ! Y from event to label (or 
\backslash
output").
\end_layout

\end_inset

 Some of techniques generally classified under ML are Artificial Neural
 Networks, Genetics Algorithm, k-Nearest Neighbor, Bayesian Networks and
 and Support Vector Machines (SVM) and has been succesfully applied to.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Standard
* overview
\end_layout

\begin_layout Standard
* Learning taxonomy
\end_layout

\begin_layout Standard
Machine learning algorithms are organized into a taxonomy, based on the
 desired outcome of the algorithm.
 Common algorithm types include:
\end_layout

\begin_layout Standard
* Supervised learning ? in which the algorithm generates a function that
 maps inputs to desired outputs.
 One standard formulation of the supervised learning task is the classification
 problem: the learner is required to learn (to approximate) the behavior
 of a function which maps a vector [X_1, X_2, 
\backslash
ldots X_N]
\backslash
, into one of several classes by looking at several input-output examples
 of the function.
 * Unsupervised learning ? which models a set of inputs: labeled examples
 are not available.
 * Semi-supervised learning ? which combines both labeled and unlabeled
 examples to generate an appropriate function or classifier.
 * Reinforcement learning ? in which the algorithm learns a policy of how
 to act given an observation of the world.
 Every action has some impact in the environment, and the environment provides
 feedback that guides the learning algorithm.
 * Transduction ? similar to supervised learning, but does not explicitly
 construct a function: instead, tries to predict new outputs based on training
 inputs, training outputs, and test inputs which are available while training.
 * Learning to learn ? in which the algorithm learns its own inductive bias
 based on previous experience.
\end_layout

\begin_layout Standard
The computational analysis of machine learning algorithms and their performance
 is a branch of theoretical computer science known as computational learning
 theory.
\end_layout

\begin_layout Standard
* differences 
\end_layout

\begin_layout Standard
* statistical machine learning
\end_layout

\begin_layout Standard
* the natural application of machine learning to information retrieval
\end_layout

\end_inset


\end_layout

\begin_layout Section
Information Retrieval
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Standard
Information Retrieval (IR) is branch of computer science whose originally
 objective was to obtain information, only text in early stages, from generally
 large amounts of data.
 Information Retrieval (IR) is very big term that comprises a lot of concepts
 regarding data manipulation, where data can be anything from text to videos.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Information Retrieval (IR) is branch of computer science whose original
 objective was to obtain information from generally large amounts of data.
 Today modern IR deals with content-based document managment task, including
 document filtering, translation, summarization, question answering among
 others.
 
\begin_inset Note Note
status collapsed

\begin_layout Standard
These last tasks can be seen as "knowledge" retrieval instead of pure 
\begin_inset Quotes eld
\end_inset

information
\begin_inset Quotes erd
\end_inset

, therefore the application of Artificial Intelligence (AI), specifically
 Machine Learning to improve or to help automatize these tasks is natural.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
IR Systems makes large amounts of text accessible to people that need the
 information.
 A user the apporaches to the system with a vague idea of what is looking
 for, so the goal of the system is to provide the information required.
 The user provides generally provides some information about what kind of
 documents is looking for, in the form of keywords.
\end_layout

\begin_layout Standard
And information retrieval system comprises several modules, Figure 
\begin_inset LatexCommand ref
reference "fig:ir-kelle"

\end_inset

 shows the basic scheme of work of a basic IR sytem.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Standard
\align center
\begin_inset Graphics
	filename images/keller-ir-diagram.png
	scale 30

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Caption

\begin_layout Standard
Information Retrieval system parts as described in [keller-THESIS]
\begin_inset LatexCommand label
name "fig:ir-kelle"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Standard
Information storage and retrieval systems make large volumes of text accessible
 to people with information needs (van Rijsbergen, 1979; Salton and McGill,
 1983).
 A person approaches such a system with some idea of what they want to find
 out, and the goal of the system is to fulfill that need.
 The person (hereafter referred to as the user) provides an outline of their
 requirements?perhaps a list of keywords relating to the topic in question,
 or even an example document.
 The system searches its database for documents that are related to the
 user?s query, and presents those that are most relevant.
 General texts relating to information retrieval and computer implementation
 of retrieval systems are Salton (1988), Frakes (1992a), Baeza-Yates (1992),
 and Witten et al.
 (1994).
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
In today's information society where digital text is becoming more and more
 available, boosted by the usage of the Internet and more and more fast
 networks and larger capacities of storage, the need of better information
 retrieval techniques becomes everyday more strong.
 
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status collapsed

\begin_layout Standard
Altough IR task has been already automated only in the last decade Machine
 Learning techniques has been applied to IR systems.
 ML techniques have shown to be very useful even surpassing in performance
 classical classification methods such as 
\begin_inset Quotes eld
\end_inset

ROCHIO
\begin_inset Quotes erd
\end_inset

 algorithm.
 In the next sections of the chapter we show how textual information is
 handled and prepared for the Text Categorization task.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Models of Text and Text Representation
\end_layout

\begin_layout Standard
Usually in a digital library database a set of words that defines the topics
 or the main characteristics of a document is manually defined.
 The main objective of this is to help the search of the physical document
 (e.g books, papers,etc.).
 This list of terms or keywords enables the user to query documents related
 to one or more concepts.
 One of the main problems of this is that only few keywords are associated
 with a defined document, and generally this assignation is made at hand,
 making it expensive and error prone.
 Genrally this querying these systems was limited to the numbers of keywords
 assigned and some metadata like title, author and dates of publication,
 being the user unable to query the whole document, making the retrieval
 task significally harder.
\end_layout

\begin_layout Standard
Nowadays in digital libraries and similar systems where the all the text
 is available in digital form, automatic keyword extraction makes sense
 because these keywords are tigthly associated with the content.
 This keywords are called indexes and the task of associating each document
 with a set of keywords is called 
\emph on
indexing
\emph default
.
 
\end_layout

\begin_layout Standard
Indexing is based is predefined group of keywords: the dictionary or vocabulary.
 As mentioned above, in a digitalized text database (also called a corpus)
 the dictionary is usually extracted from the set of words present in the
 corpus, and the expensive manual indexing can be avoided by representing
 the documents by the dictionary terms they are made of.
 Given that all the full text is avalaible for querying it, now problem
 is to find the best way of representing the text in order to extract the
 more important words, that is, the keywords that define it.
 
\end_layout

\begin_layout Standard
During the last few decades many representation forms have been developed
 ranging from simple word appareance binary representation to a concept
 
\begin_inset LatexCommand cite
key "deerwester90indexing"

\end_inset

, probabilistic 
\begin_inset LatexCommand cite
key "keller-theme"

\end_inset

 and even Neural Networks 
\begin_inset LatexCommand cite
key "DBLP:conf/icann/KellerB05"

\end_inset

 based ones.
 
\end_layout

\begin_layout Standard
The most common approachg to automatically extact the index terms from a
 corpues is called vector space model 
\begin_inset LatexCommand cite
key "361220"

\end_inset

.
 In this model each document 
\begin_inset Formula $d$
\end_inset

 is representes as a vector
\begin_inset Formula $(\theta_{1},...,\theta_{M})$
\end_inset

 where 
\begin_inset Formula $\theta_{j}$
\end_inset

 is a funtion of the frecuancy in 
\begin_inset Formula $d$
\end_inset

 of the 
\begin_inset Formula $j^{th}$
\end_inset

 word of a chosen dictionary 
\begin_inset Formula $M$
\end_inset

.
 
\end_layout

\begin_layout Standard
Based in this definition various forms of 
\begin_inset Formula $\theta(j)$
\end_inset

have been defined:
\end_layout

\begin_layout Description
Binary In which 
\begin_inset Formula $\theta_{j}$
\end_inset

 is equal to 1 if the 
\begin_inset Formula $j^{th}$
\end_inset

 word appears in the document or 0 otherwise.
\end_layout

\begin_layout Description
Frecuency In which 
\begin_inset Formula $\theta_{j}$
\end_inset

 is equal to the number of times 
\begin_inset Formula $j^{th}$
\end_inset

 word appears in the document.
\end_layout

\begin_layout Description

\emph on
tf-idf
\emph default
 In which 
\begin_inset Formula $\theta_{j}$
\end_inset

 is equal to the 
\emph on
tf-idf
\emph default
 formula:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\theta_{j}=tf_{j}(d)\centerdot log(\frac{N}{df_{j}}),\,\,\,\,\forall j\in\{1,\ldots,M\}\label{eq:tf-idf}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The equation 
\begin_inset LatexCommand ref
reference "eq:tf-idf"

\end_inset

 was defined proposed by Salton and Buckley 
\begin_inset LatexCommand cite
key "866292"

\end_inset

 and it is the more complex of the three.
 the 
\begin_inset Formula $tf_{j}(d)$
\end_inset

 (term frecuency) is the number of times that the 
\begin_inset Formula $j^{th}$
\end_inset

 word appears in the document 
\begin_inset Formula $d$
\end_inset

, 
\begin_inset Formula $df_{j}$
\end_inset

 is the number of documents the term appears in all the corpus and 
\begin_inset Formula $N$
\end_inset

 is the total number of documents in the corpus.
 The main reason behind this formulation is that in the context of document
 retrieval it is considered that words appearing too frequently across the
 corpus may not be discriminant, therefore this formula gives more importance
 to terms appearing more frecuently in the documents while penalizing the
 ones that appears in to many documents.
 The reader may have noticed that this representation of text does not take
 into account the actual order of the words inside the document, despite
 this, vector space model have performed well and it still used with some
 variations in the majority of IR systems.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Standard
In this model, each document d is represented as a vector (1, ..., M), where
 j is a function of the frequency in d of the jth word wj of a chosen dictionary
 of size M.
 Note that the VSM does not take into account the order of the words in
 the document.
 For that reason it is often referred to as bag-of-words representation.
 The simplest choice for the weights .
 corresponds to the binary indexing, where: j = ( 1 if the word wj appears
 in d, 0 otherwise 8j 2 {1, .
 .
 .
 ,M}.
 The more complex term frequency inverse document frequency (TFIDF) weighting
 scheme was proposed by Salton and Buckley (1988) in the context of document
 retrieval where it is considered that words appearing too frequently across
 the corpus may not be discriminant.
 In its standard form it is defined by the following formula:
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Standard
approaches for feature selection, others than the design of a stop-list
 and the stemming of words.
 These approaches attempt to select from the original vocabulary V, the
 sub-set V0 of terms (with V0 V) that, when used for document representation,
 provides the best results.
 As stated in Sebastiani (2002), the choice of this subset is generally
 done by a filtering approach, that is selecting the terms that reach the
 highest scores according to a function that measures the ‚Äúimportance‚Äù of
 each term for the task.
 This function can simply be the document frequency (Yang and Pedersen,
 1997; Dumais et al., 1998), or a more complex information-theoretic function
 such as information gain (Yang and Pedersen, 1997; Yang and Liu, 1999)
 , mutual information (Yang and Pedersen, 1997; Dumais et al., 1998), chi-square
 (Schutze et al., 1995; Yang and Pedersen, 1997; Yang and Liu, 1999), etc.
 Nevertheless, Joachims (1998) shows that the performance of a system trained
 without the ‚Äúbest‚Äù features according to mutual information criterion are
 still better than a random system.
 This result proves that even the ‚Äúworst‚Äù features still contain discriminant
 information, and thus suggest that all features should be selected, when
 possible.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
the vector space model representation described above may seem simplistic
 in comparision with the more complex represenation mentioned first, nevertheles
s, it seems that for some tasks like text categorization, the classical
 tf-idf works just well and more complex representations of the text do
 not significantly affect the performance of the methods.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Standard
\align center
\begin_inset Graphics
	filename images/joachims-text-vect.jpg
	lyxscale 50
	scale 20

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Caption

\begin_layout Standard
Representing text as a feature vector 
\begin_inset LatexCommand cite
key "Joachims98"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Feature Selection and Dimensionality Reduction
\end_layout

\begin_layout Standard
Due to the fact that the represenation of the text can have impact in any
 retrieval task, any strategy to enhace the representation of text should
 be studied.
 With the years, many techniques have been developed in the field of Information
 Retrieval in order to get a better performance on the representation of
 the text made.
 IR research shows us that the word stems work well as representation units
 
\begin_inset LatexCommand cite
key "Joachims98"

\end_inset

.
 The word stemming process (sometimes called term coflation) consists in
 removing the case and flection information from a word 
\begin_inset LatexCommand cite
key "Porter80"

\end_inset

.
 For example words as 
\begin_inset Quotes eld
\end_inset

engine
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

engineer
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

engineering
\begin_inset Quotes erd
\end_inset

 becomes the same term 
\begin_inset Quotes eld
\end_inset

engin
\begin_inset Quotes erd
\end_inset

.
 Generally this techniques are applied to the document before the transforming
 it into a term vector representation, thus making the resulting document
 vector notably smaller.
\end_layout

\begin_layout Standard
Another technique used to avoid innecesary large feature vectors is removed
 the so-called stop-words, or the words that for their common use is known
 that are not significant for the text.
 Words like 
\begin_inset Quotes eld
\end_inset

and
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

or
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

of
\begin_inset Quotes erd
\end_inset

, etc are removed from the original text.
\end_layout

\begin_layout Standard
There are other approaches for selecting the most important features other
 than the described above, these approaches try to select the most important
 features (the 
\begin_inset Quotes eld
\end_inset

best
\begin_inset Quotes erd
\end_inset

 words) from the dictionary 
\begin_inset Formula $M$
\end_inset

, creating a subset 
\begin_inset Formula $M0$
\end_inset

, that used for document representartion provides better results.
 However Joachims 
\begin_inset LatexCommand cite
key "Joachims98"

\end_inset

 shows that the 
\begin_inset Quotes eld
\end_inset

worst
\begin_inset Quotes erd
\end_inset

 features still contain importan information, therefore suggest that all
 features should be selected, when it is possible.
\end_layout

\begin_layout Section
Text Categorization
\begin_inset LatexCommand label
name "sec:Text-Categorization"

\end_inset


\end_layout

\begin_layout Standard
The goal of text categorization (TC) is to automatically assign, for each
 documents, categories selected among a predefined set.
 In opposition to the machine learning typical multi-class classification
 task which aims at attributing one class among the possible ones to each
 example, documents in a text categorization task may belong to several
 categories.
 Sebastiani 
\begin_inset LatexCommand cite
key "Sebastiani02"

\end_inset

 formally defines TC as task of assigning a Boolean to each pair 
\begin_inset Formula $\left\langle d_{j},c_{j}\right\rangle \in\mathcal{D}\,\, x\,\mathcal{\, C}$
\end_inset

 where 
\begin_inset Formula $\mathcal{D}$
\end_inset

 is a domain of documents and 
\begin_inset Formula $\mathcal{C}=\{c_{1},\ldots,c_{|c|}\}$
\end_inset

 is a set of predefined categories.
 a true value assigned to the pair 
\begin_inset Formula $\left\langle d_{j},c_{j}\right\rangle $
\end_inset

 indicates the decision of filling 
\begin_inset Formula $d_{j}$
\end_inset

 under category 
\begin_inset Formula $c_{j}$
\end_inset

, and a false value the decision of not filling it under that category.
 More formally the task is to approximate a unknown target function 
\begin_inset Formula $\breve{\Phi}:\mathcal{D}\,\, x\,\mathcal{\, C}\rightarrow\{T,F\}$
\end_inset

 using 
\begin_inset Formula $\Phi:\mathcal{D}\,\, x\,\mathcal{\, C}\rightarrow\{T,F\}$
\end_inset

 such that 
\begin_inset Formula $\Phi$
\end_inset

and 
\begin_inset Formula $\breve{\Phi}$
\end_inset

 
\begin_inset Quotes eld
\end_inset

coincide as much as possible
\begin_inset Quotes erd
\end_inset

 
\begin_inset LatexCommand cite
key "Sebastiani02"

\end_inset

.
 
\end_layout

\begin_layout Standard
A text categorization task can be performed in many ways, for instace one
 might want to classify a document in just one category (single label case
 or not overlapping categories) or from 
\begin_inset Formula $0$
\end_inset

 to 
\begin_inset Formula $|\mathcal{C}|$
\end_inset

 categories (multilabel case or overlapping categories).
 A special case of sigle label TC is the binary TC in which each 
\begin_inset Formula $d_{j}$
\end_inset

 is assigned to category 
\begin_inset Formula $c_{i}$
\end_inset

 or its complement 
\begin_inset Formula $\widetilde{c}_{i}$
\end_inset

.
 
\end_layout

\begin_layout Standard
It is possible to percieve the multilabel case as different problem but
 one can express the multilabeled TC task for a set of categories 
\begin_inset Formula $\mathcal{C}=\{c_{1},\ldots,c_{k}\}$
\end_inset

 as a group of 
\begin_inset Formula $k$
\end_inset

 binary case problems, this requiers that categories be stochastic independent
 or each other altough some recent development in structured classification
 
\begin_inset LatexCommand cite
key "Tsochantaridis/etal/05a"

\end_inset

 might be useful to tackle multilabel interdependant TC task.
 In this work we are going to deal with binary categorization tasks because
 as stated by Sebastiani 
\begin_inset LatexCommand cite
key "Sebastiani02"

\end_inset

 many important TC applications are in fact binary categorization problems
 and moreover, as described above we can solve multilabel TC as many binary
 TC task assuming independent categories, which id a valid assumption for
 most of the problems.
 
\end_layout

\begin_layout Subsection
Performance Measures
\end_layout

\begin_layout Standard
One question that arises after reading the definition of TC is how to measure
 the how well the classifier function
\begin_inset Formula $\breve{\Phi}$
\end_inset

 (the approximation or hypothesis) matches the unkwon target function 
\begin_inset Formula $\Phi$
\end_inset

 (efectivness).
 This is generally measured in terms of the classic IR metrics of precision
 
\begin_inset Formula $P$
\end_inset

 and recall 
\begin_inset Formula $R$
\end_inset

 but adapted to the specific case of TC.
 
\begin_inset Formula $P$
\end_inset

 is defined as the probability that if a given a random document 
\begin_inset Formula $d_{x}$
\end_inset

 is classified under the category 
\begin_inset Formula $c_{i}$
\end_inset

 the classificaion is correct (
\begin_inset Formula $P(\breve{\Phi}(d_{x},c_{i})=T\,\,|\,\, P(\Phi(d_{x},c_{i})=T)$
\end_inset

).
 Analogously 
\begin_inset Formula $R$
\end_inset

 is defined as the probability that if a random document 
\begin_inset Formula $d_{x}$
\end_inset

 is ought to be classified under 
\begin_inset Formula $c_{i}$
\end_inset

, this decision is taken (
\begin_inset Formula $P(\Phi(d_{x},c_{i})=T\,\,|\,\, P(\breve{\Phi}(d_{x},c_{i})=T)$
\end_inset

).
 
\end_layout

\begin_layout Standard
In binary TC, with we can express the concepts of precisition and recall
 mathematicaly using the numbers of true positive (
\begin_inset Formula $TP$
\end_inset

), false positive (
\begin_inset Formula $FP$
\end_inset

), true negative (
\begin_inset Formula $TN$
\end_inset

) and false negative (
\begin_inset Formula $TN$
\end_inset

) documents classified.
 The true of false value means that the classification was performed correctly
 or not, and the positive or negative define wheter it belonged to the evaluated
 category or not.
 Using these definitions we can write precision 
\begin_inset Formula $P$
\end_inset

 and recall 
\begin_inset Formula $R$
\end_inset

 as:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
P=\frac{TP}{TP+FP} & ,\, & R=\frac{TP}{TP+FN}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
For multilabeled classification this only would be local measures for a
 single category c, in that case the local measures are averaged using different
 methods such as 
\emph on
microaveraging 
\emph default
and 
\emph on
macroaveraging
\emph default
 
\begin_inset LatexCommand cite
key "Sebastiani02"

\end_inset

.
\end_layout

\begin_layout Standard
The values of precisition and recall for TC vary in an inverse relation,
 for that reason many articles about TC uses break-even point of 
\begin_inset Formula $P$
\end_inset

 and 
\begin_inset Formula $R$
\end_inset

 
\begin_inset LatexCommand cite
key "Joachims98,DumaisPHS98,Lewis_et_al_2004"

\end_inset

 which generally implies to modify in some degree the hypothesis.
 Another approach is to use the 
\begin_inset Formula $F1$
\end_inset

 measure 
\begin_inset LatexCommand cite
key "citeulike:543360,Manevitz_et_al2002,112471"

\end_inset

 which combines both precision and recall and is defined as:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
F1=\frac{2RP}{P+R}\]

\end_inset


\end_layout

\begin_layout Standard
This This implies that 
\begin_inset Formula $F1$
\end_inset

, like 
\begin_inset Formula $P$
\end_inset

 and 
\begin_inset Formula $R$
\end_inset

 is bounded by 1 and the best results under this measure are the higher
 values.
 
\end_layout

\begin_layout Subsection
Applications of Text Categorization
\end_layout

\begin_layout Standard
TODO: List Applications such as Text Filtering, Automated metadata extraction,
 Recommender Systems, Web Search Improvements.
\end_layout

\begin_layout Subsection
Machine Learning Text Categorization Techniques
\end_layout

\begin_layout Standard
TODO: List ML Techniques suchas Naive Bayes, SVM, DTress among othes and
 shows some benchmarks.
\end_layout

\begin_layout Section
Paralell Machine Learning
\begin_inset LatexCommand label
name "sec:Paralell-Machine-Learning"

\end_inset


\end_layout

\begin_layout Standard
Only recently the topic of parallel implementation of machine learning algorithm
s have come to importance, altough it is rarely addressed at major machine
 learning conferences, many factors points out that the importance of this
 factor will increase.
 In his popular weblog
\begin_inset Foot
status collapsed

\begin_layout Standard
Machine Learning (Theory) - http://hunch.net/
\end_layout

\end_inset

 John Langford
\begin_inset Foot
status collapsed

\begin_layout Standard
http://hunch.net/~jl/
\end_layout

\end_inset

 describes the main tendencies of computation that impulse the necessity
 of paralell implementation of machine learning algorithms:
\end_layout

\begin_layout Enumerate
Larger datasets available due to the avalability of more sensors on the
 internet, making processor power a need in other to handle these large
 amounts of information.
\end_layout

\begin_layout Enumerate
Serial processor speedups are stalled as the result of no having the ability
 to drive chips at ever higher clock rates without excessingly incresing
 the cost.
 The new tendency for improving processor speed is to have more cores per
 processor 
\begin_inset LatexCommand cite
key "1069628|Geer05"

\end_inset

, that is, creating multicore processors, for example, IBM cell processor
\begin_inset Foot
status collapsed

\begin_layout Standard
http://www.research.ibm.com/cell/
\end_layout

\end_inset

 has 9 cores.
\end_layout

\begin_layout Standard
In addition, since 2005 there is low-cost parallel hardware aimed to desktop
 users 
\begin_inset LatexCommand cite
key "Marowka07"

\end_inset

 making even more important to take avantage of this architecture and learn
 to use it the best way.
 
\end_layout

\begin_layout Standard
With all these precedents the following question arises: how take advantage
 of this trend and use all the parallel computing power of new processor
 for solving machine learning problems? We are in the beginning of the multicore
 era and there is no easy answer to this question, mostly because there
 is not even an unified general approach to take avantage ot this arquitecture,
 altough some basic strategies can be described.
\end_layout

\begin_layout Standard
The basic approach is to run the same algorithm with different parameters
 on different processors.
 Cluster software like Open Mosix, Condor might be used in this situation.
 As noticed by Langford, this particular approach does not speed up the
 run of a particular machine learning algorithm.
 Other approaches includes the descomposition of the algorithm into statisitical
 queries 
\begin_inset LatexCommand cite
key "167200"

\end_inset

 and paralellize the queries over the the samples, for example using map-reduce
 
\begin_inset LatexCommand cite
key "conf/nips/ChuKLYBNO06"

\end_inset

.
 The last and most difficult way is to develop fine-grained parallism, which
 is the way brain actually works.
 
\end_layout

\begin_layout Standard
We have discussed the evident speed gain using parallel methods for running
 but the question whether the a paralell algorithm fine-grained paralellized
 implementation can achieve better performance is still unanswered.
 Another question that arises is if there are methods that not necesarilly
 incur in the complexity of disecting a specific algorithm and then paralleling
 and still harness parallel architectures.
 Graf et Al.
 
\begin_inset LatexCommand cite
key "GrafCBDV04"

\end_inset

 presents the Cascade SVM, an approach for parallelizing the SVM without
 modifying the SVM problem at all but using a pyramid-like array of SVM
 subproblem that are joined in each level until a the solution is found.
 This approach is chosen for the experiments of this work and will be discussed
 in depth in chapter 
\begin_inset LatexCommand ref
reference "cha:Experiments-and-Results"

\end_inset


\end_layout

\begin_layout Chapter
Text Categorization With SVM
\end_layout

\begin_layout Section
Support Vector Machines
\end_layout

\begin_layout Standard
Support Vector Machines (SVMs) 
\begin_inset LatexCommand cite
key "Vapnik98"

\end_inset

 are powerful classification and regresion tools that have been widely applied
 in the solutions of many problem, generally yielding comparable or even
 better performance that other algortihms.
 
\end_layout

\begin_layout Standard
The SVM algorithm is based on the concept of VC Dimension
\begin_inset LatexCommand cite
key "vapnik71uniform"

\end_inset

 and on the principle of empirical risk minimization developed by Vapnik
 
\begin_inset LatexCommand cite
key "Vapnik99,vapnik71uniform"

\end_inset

.
 Roughly, the VC dimension is a metric of how complex a classifier machine
 is, complex classifier has more capacity to fit the training data, thus,
 overfitting is more likely to occur, therefore, is prefered a classifier
 with minimum VC Dimension.
 
\begin_inset Note Note
status collapsed

\begin_layout Standard
Shoud I add the formula of Empirical Risk Minimization
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The basic idea of empirical risk minimization principle is to find an hypothesis
 
\begin_inset Formula $s$
\end_inset

 from an hypothesis space 
\begin_inset Formula $S$
\end_inset

 for which the lowest probability of error is guaranteed for a given set
 of training examples.
 
\end_layout

\begin_layout Standard
For linear classification problems this is equivalent to find the discrimination
 function that maximizes the distances within the classes, assuring the
 lowest probability of error
\begin_inset LatexCommand cite
key "citeulike:368926|Haykin1998"

\end_inset

.
 The aim of the Support Vector classification is to devise a computationally
 efficient way of learning the optimal separating hyperplanes in high dimensiona
l feature space 
\begin_inset LatexCommand cite
key "citeulike:114719|Cristianini2000introSVM"

\end_inset

, 
\begin_inset Note Note
status collapsed

\begin_layout Standard
efining optimal hyperplane as the one that has both, the minimum empirical
 risk and VC dimension.
 .
 
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Finding the Optimal Hyperplane
\end_layout

\begin_layout Standard
Let us consider a training sample 
\begin_inset Formula $\{(x_{i},y_{i})\}_{i=1}^{N}$
\end_inset

, where x is a point in 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 that belongs to the group of training examples or input patterns for 
\begin_inset Formula $i$
\end_inset

th example and 
\begin_inset Formula $d_{i}$
\end_inset

 is the corresponded, desired response.
 In this case we are gonna simplifly the setting by stating that 
\begin_inset Formula $d_{i}$
\end_inset

 can only take two values 
\begin_inset Formula $\{+1,-1\}$
\end_inset

 for the positive and negative classification case respectively.
 This is case of a binary classification problem, for multiclass classification
 problem as described back in [INT-REF HERE] we can transform the problem
 in different binary classification ones.
 We also assume that the class represented by 
\begin_inset Formula $y_{i}=\{+1,-1\}$
\end_inset

 are lineary separable.
 The equation of a decision hyperplane that separates the data is:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
w^{T}x+b=0\label{eq:}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Standard
\align center
\begin_inset Graphics
	filename images/svms-1-basic-2.png
	lyxscale 50
	scale 25

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Caption

\begin_layout Standard
The red circles and blue x's are the training data, the red circles for
 the 
\begin_inset Formula $y_{i}=-1$
\end_inset

 class and the blue x's for the 
\begin_inset Formula $y_{i}=+1$
\end_inset

 class.
 The data inside the grey circles are the support vectors that maximize
 the distance between the data and the hyperplane defined by 
\begin_inset Formula $w$
\end_inset

, a normal vector to the hyperplane and the offset 
\begin_inset Formula $b$
\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $x$
\end_inset

 is an input vector, 
\begin_inset Formula $w$
\end_inset

 is the weight vector, normal to the separation hyperplane, and 
\begin_inset Formula $b$
\end_inset

 is the offset or bias:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
 & w^{T}x+b\geq0\mbox{\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,}for\,\,\,\,\,\,\, y_{i}=+1\\
 & w^{T}x+b<0\mbox{\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,}for\,\,\,\,\,\,\, y_{i}=-1\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
For a given vector 
\begin_inset Formula $w$
\end_inset

 and offset 
\begin_inset Formula $b$
\end_inset

 the distance between the the hyperplane defined in and a point 
\begin_inset Formula $x$
\end_inset

 is called margin of separation represented by the equation
\begin_inset LatexCommand ref
reference "eq:"

\end_inset

.
\end_layout

\begin_layout Standard
Then we can write the discriminat function:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
{g(x)=w}^{T}x+b\label{eq:2}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
An the classification function as:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
y=sign\{w^{T}x+b\}\label{eq:3}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Equation 
\begin_inset LatexCommand ref
reference "eq:2"

\end_inset

 gives a measure of the distance from x to the optimal hyperplane, and 
\begin_inset LatexCommand ref
reference "eq:3"

\end_inset

 assing a category for a specific point.
 
\end_layout

\begin_layout Standard
As described above, the complexity of the classifier should be minimized
 in order to achieve a good generalization, for linear classifier this equivalen
t lowering that complexity is to maximize the margin of separation between
 the points and the hyperplane.
 An approach to find this separator is to maximize the margin between two
 parallel supporting planes, if we define the margin as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
 & w^{T}x+b=\,\,\,1\\
 & w^{T}x+b=-1\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Then the margin of separation is equal then to 
\begin_inset Formula $\gamma=\frac{2}{w}$
\end_inset

 .
 Figure [TALES#1] shows this approach.
 Now our problem becomes maximize 
\begin_inset Formula $\frac{2}{w}$
\end_inset

 which is equivalent to minimize 
\begin_inset Formula $||w||_{2}/2$
\end_inset

 in the following quadratic programming problem:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray}
 & \underset{w,b}{min} & \frac{1}{2}||w||^{2}\nonumber \\
s.t &  & y_{i}(w^{T}x_{i}+b)\geq1\label{eq:min0.5w2}\end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
Condition 
\begin_inset Formula $d_{i}(w^{T}x_{i}+b)\geq1$
\end_inset

 spcify that all the data points have to be classified correctly.
 In order to solve this problem we introduce the lagrangian formulation:
\end_layout

\begin_layout Standard
\SpecialChar \-

\begin_inset Formula \begin{eqnarray}
\,\underset{\alpha}{max}\,\underset{w,b}{min}\,\, J(w,b,\alpha) & = & \frac{1}{2}w^{T}w-\sum_{i=1}^{N}\alpha_{i}[y_{i}(w^{T}x_{i}+b)-1]\label{eq:lagrangian}\\
s.t &  & \alpha_{i}\geq0\nonumber \end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
Now the conditions of optimality for 
\begin_inset Formula $J(w,b,\alpha)$
\end_inset

 are given by:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\,\,\,\,\,\frac{\delta J(w,b,\alpha)}{\delta w}=0\longrightarrow w=\sum_{i=1}^{N}\alpha_{i}y_{i}x_{i}\label{eq:weqsumadx}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\frac{\delta J(w,b,\alpha)}{\delta b}=0\longrightarrow\sum_{i=1}^{N}\alpha_{i}y_{i}=0\label{eq:sumadeq0}\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Replacing 
\begin_inset LatexCommand ref
reference "eq:weqsumadx"

\end_inset

 and 
\begin_inset LatexCommand ref
reference "eq:sumadeq0"

\end_inset

 into 
\begin_inset LatexCommand ref
reference "eq:lagrangian"

\end_inset

 and defining the cost function as 
\begin_inset Formula $J(w,b,\alpha)=Q(\alpha)$
\end_inset

 yields:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray}
min\, Q(\alpha) & = & \sum_{i=1}^{N}\alpha-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j}\label{eq:minq}\\
s.t &  & \alpha_{i}\geq0\,\, and\,\,\,\sum_{i=1}^{N}\alpha_{i}y_{i}=0\nonumber \end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
Which is a quadratic programming problem (QP).
 Notice that the dual problem is cast entirely in terms of the training
 data.
 The fact that this is a QP problem is one of the streght of this methodology
 and also a weakness.
 It's a streght because as 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
opposed to others techniques such as Neural Networks, the optimization problem
 is convex and a global optimum is always reached in a finite amount of
 time 
\begin_inset LatexCommand cite
key "Joachims98"

\end_inset

.
 On the other hand, a QP is a computationally expensive problem, this property
 has led to the development of both, novel ways of solving the problem described
 in equation 
\begin_inset LatexCommand ref
reference "eq:minq"

\end_inset

 and alternative formulations for solving the same optimization problem
 
\begin_inset LatexCommand cite
key "Suykens-et-al2002,Joachims/06a"

\end_inset

, with variying degrees of performace.
 The 
\begin_inset Formula $x{}_{i}$
\end_inset

's whose 
\begin_inset Formula $\alpha{}_{i}$
\end_inset

 are not defines are the support vectors, that is they are on the hyperplanes
 that defines the maximum margin separator.
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
When all points cannot be shattered by one Hyperplane an error bound is
 added in the form of a nonnegative slack variable, the primal problem described
 in 
\begin_inset LatexCommand ref
reference "eq:min0.5w2"

\end_inset

 becomes:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray}
 & \underset{w,b}{min} & \frac{1}{2}||w||^{2}+C\sum_{i=1}^{N}\xi_{i}\nonumber \\
 &  & y_{i}(w^{T}x_{i}+b)+\xi_{i}\geq1\label{eq:min0.5w2plusc}\\
s.t\nonumber \\
 &  & \xi_{i}\geq0\,\,\,\, i=1,...,m\nonumber \end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $C$
\end_inset

 is a user-specified positive parameter and depends on the data, empirical
 ways of finding the adequate C exist.
 After following a similar procedure to the one that lead us to 
\begin_inset LatexCommand ref
reference "eq:minq"

\end_inset

 we now have:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray}
min\, Q(\alpha) & = & \sum_{i=1}^{N}\alpha-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j}\nonumber \\
s.t\nonumber \\
 &  & \alpha\geq0\nonumber \\
 &  & \sum_{i=1}^{N}\alpha_{i}y_{i}=0\label{eq:minwithc}\\
 &  & 0<\alpha_{i}\leq C\,\,\,\, i=1,...,m\nonumber \end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
The C param defines the error bound, and in plain words in states how 
\begin_inset Quotes eld
\end_inset

hard
\begin_inset Quotes erd
\end_inset

 the classifier should try to correctly classify a given point before removing
 it from the training set.
 Once calculated the 
\begin_inset Formula $\alpha_{i}$
\end_inset

 the optimal weight vector 
\begin_inset Formula $w_{o}$
\end_inset

 and offset 
\begin_inset Formula $b_{o}$
\end_inset

 can be calculated as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray}
 & w_{o}= & \sum_{i=1}^{N}\alpha_{i}y_{i}x_{i}\nonumber \\
\label{eq:w0b0}\\ & b{}_{o}= & 1-w_{o}^{T}x^{(s)}\nonumber \end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $x^{(s)}$
\end_inset

 is any support vector.
 We can also write the discriminat function as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
y=sign\{\sum_{i=1}^{N}d_{i}\alpha_{i}x_{i}^{T}x+b_{0}\}\]

\end_inset


\end_layout

\begin_layout Standard
Many times a linear separator is not enough to classify the data properly,
 geometrically speaking, a more complex surface is needed in order to separate
 the data correctly.
 In order to classify the data a non-liner mapping into a high-dimensional
 feature space where the data is lineary separable, the Cover's theorem
 on separability 
\begin_inset LatexCommand cite
key "Cover65a|citeulike:1803784"

\end_inset

 supports this procedure .
 in order to accomplish this, SVM uses the so called 
\begin_inset Quotes eld
\end_inset

Kernel Trick
\begin_inset Quotes erd
\end_inset

 which consist in the usage of a Kernel function that maps the input data
 into a higher dimensional space.
 The reader may consult the appropiate literature 
\begin_inset LatexCommand cite
key "citeulike:368926|Haykin1998,citeulike:755348|Heibrich2001Lkernel,citeulike:114719|Cristianini2000introSVM"

\end_inset

 for more information on this method.
 The dual for formulation using this approach is:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray}
min\, Q(\alpha) & = & \sum_{i=1}^{N}\alpha-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}d_{i}d_{j}K(x_{i},x_{j})\nonumber \\
s.t\nonumber \\
 &  & \alpha\geq0\nonumber \\
 &  & \sum_{i=1}^{N}\alpha_{i}d_{i}=0\label{eq:minwithckernel}\\
 &  & 0<\alpha_{i}\leq C\,\,\,\, i=1,...,m\nonumber \end{eqnarray}

\end_inset


\end_layout

\begin_layout Standard
And the discriminant function is:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
y=sign\{\sum_{i=1}^{N}d_{i}\alpha_{i}K(x_{i},x)+b_{0}\}\]

\end_inset


\end_layout

\begin_layout Standard
There are many available Kernel functions, and they depend on the characteristic
s of the input data.
 In this work we are going to use a linear SVM as defined in equation 
\begin_inset LatexCommand ref
reference "eq:minwithc"

\end_inset

, which also can be viewed as 
\begin_inset LatexCommand ref
reference "eq:minwithckernel"

\end_inset

 equation using the kernel function: 
\begin_inset Formula $K(x_{i,}x_{j})=x_{i}^{T}x_{j}$
\end_inset

.
 
\end_layout

\begin_layout Subsection
Transductive Learning for SVM
\end_layout

\begin_layout Standard
So far with the formulations described in the last section we are using
 what is called inductive learning, in other words we start from a set of
 examples, and based on them we try to find a classificator that classifies
 correctly the data.
 This is called Inference learning, that is, going from particular examples
 to a general hypothesis.
 The transductive setting on the other hand tries to use the known distribution
 of the test data in order produce a classifier.
 [TODO]
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Standard
\align center
\begin_inset Graphics
	filename images/joachims-algorithm.jpg
	scale 42

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Caption

\begin_layout Standard
Algorithm for training Transductive Support Vector Machines 
\begin_inset LatexCommand cite
key "Joachims99c"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\end_layout

\end_inset


\end_layout

\begin_layout Standard
[TODO Description of the algorithm]
\end_layout

\begin_layout Section
SVM and Text Categorization
\end_layout

\begin_layout Standard
The usage of SVM was first introduced by Joachims 
\begin_inset LatexCommand cite
key "Joachims98,Joachims99c"

\end_inset

 and similar setups have been used in posterior literature
\begin_inset LatexCommand cite
key "DumaisPHS98"

\end_inset

.
 the TC task using SVM can bee seen geometrically as finding an hyperplane
 that separates two grous of points has (...) [TODO]
\end_layout

\begin_layout Section
Paralell Methods for Training SVM
\end_layout

\begin_layout Subsection
Transductive Parallel Linear SVM
\end_layout

\begin_layout Chapter
Paralell Methods for Training Transductive SVM 
\begin_inset LatexCommand label
name "cha:Experiments-and-Results"

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Solving the transductive SVM using the algorithm described by joachims implies
 solve many times an SVM inductive optimization problem.
 The algorithm improves the objective function by swintching the labels
 interactively of two unlabeled data points 
\begin_inset Formula $x_{i}$
\end_inset

 and 
\begin_inset Formula $x_{j}$
\end_inset

 with 
\begin_inset Formula $\xi_{i}+\xi_{j}>2$
\end_inset

.
 It uses two nested loops to optimize a TSVM which solves a quadratic programmin
g everytime it changes a label from an unlabeled data.
 The convergence of the nested loop relies in the fact that there is only
 a finite number of finite number 
\begin_inset Formula $2^{U}$
\end_inset

 of labeling of 
\begin_inset Formula $U$
\end_inset

 unlabeled points even if it is unlikely that all of them are examinated
 
\begin_inset LatexCommand cite
key "1248609"

\end_inset

.
 However because the heuristic only swaps the label of two unlabeled examples
 at each nested loop iteration it might need (and in fact it does) many
 iterations to reach a minimum which makes it intractable for big datasets
 in practice.
\end_layout

\begin_layout Standard
TSVM in the worst case have complexity of 
\begin_inset Formula $O(L+U)^{3}$
\end_inset

 with 
\begin_inset Formula $U$
\end_inset

 labeled points, altough it generally scales to square in most practical
 cases 
\begin_inset LatexCommand cite
key "Joachims/99a"

\end_inset

 thus still intractable for large datasets.
\end_layout

\begin_layout Standard
In order to optimize this kind problem one can try to modify the formulation,
 for instance example solving it in the primal rather than in the dual with
 an alternate technique altough the question of how widely applicable are
 these methods are remains to bee seen.
 or use any of the general approaches described in section 
\begin_inset LatexCommand ref
reference "sec:Paralell-Machine-Learning"

\end_inset

 that makes use of the computation power given by the new multicore processors.
 
\end_layout

\begin_layout Standard
For the specific case of SVM and TSVM one could try to modify the problem
 in a way that makes it easily parallelizable 
\begin_inset LatexCommand cite
key "1248601"

\end_inset

 Generally this is acomplished through the use of decomposition technique
 of some kind.
 Another approach is to use a mixture of SVM 
\begin_inset LatexCommand cite
key "citeulike:935557|Collobert2002"

\end_inset

 in order to to train each of them with a subset and them using the mixture
 of expert approach 
\begin_inset LatexCommand cite
key "Jacobs:Jordan:Nowlan:Hinton91"

\end_inset

 which is easily paralellizable.
 All these approach modify in some manner the original formulation of the
 problem.
\end_layout

\begin_layout Standard
A recent approach called the Cascade SVM consist in an array of SVM solvers
 arranged in pyramid form.
 This structure allows an easy parallelization and does not modify the original
 formulation at all 
\begin_inset LatexCommand cite
key "GrafCBDV04,ZhangZY05"

\end_inset

.
 This is the approach selected for the parallelization of TSVM because there
 is no need to modify the original quadratic programming probel and is straightf
orward to implement altough some small modifications are needed in order
 to be fully adapted to the transuductive algorithm described by Joachims.
 
\end_layout

\begin_layout Section
The Cascade SVM
\end_layout

\begin_layout Standard
The Cascade SVM was described by Graf et al 
\begin_inset LatexCommand cite
key "GrafCBDV04"

\end_inset

 and further explored by Zhang et al 
\begin_inset LatexCommand cite
key "ZhangZY05"

\end_inset

 and is based in the strategy of early elimination of non-support vectors
 from the training set 
\begin_inset LatexCommand cite
key "Joachims/99a"

\end_inset

.
 The Cascade SVM is a distributed architecture wjere smaller optimization
 problems are solved independently making them easily parallelizable.
 The results of this smaller problems are then esembled in a way that is
 ensured that it eventually converges to globally optimal solution.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Standard
\align center
\begin_inset Graphics
	filename images/graf-svm-cascade.jpg
	scale 50

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Caption

\begin_layout Standard
Schematic of a Cascade architecture 
\begin_inset LatexCommand cite
key "GrafCBDV04"

\end_inset


\end_layout

\end_inset


\begin_inset LatexCommand label
name "fig:svm-cascade"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In order to find a minimum using this approach the problem is initialized
 with a number of idependent smaller optimization whose results are combined
 in later stages in a hierarchical fashion as shown in figure 
\begin_inset LatexCommand ref
reference "fig:svm-cascade"

\end_inset

.
 The data is split into subsets and everyone of them is evaluated for to
 find support vectors in the first layer, then the results are combined
 two-by-two and entered as training set for the next layer.
 Often a single pass through the cascade produces satsifactory approaches
 
\end_layout

\begin_layout Subsection
Cascade Transductive SVM
\end_layout

\begin_layout Subsection
Data Sets
\end_layout

\begin_layout Section
Experiments and Results
\end_layout

\begin_layout Section
Conclusions
\end_layout

\begin_layout Standard
\begin_inset LatexCommand bibtex
options "acm"
bibfiles "bibblio"

\end_inset


\end_layout

\end_body
\end_document
