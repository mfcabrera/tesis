With the surge in digital text media, text classification has become increasingly important.
Text classification techniques can assist in junk e-mail detection [SDHH98], allow
medical doctors to more rapidly find relevant research [HBLH94], aid in patent searches
[Lar99], improve web searches [CD00], and serve as a backend in a multitude of other
applications. The interested reader should see Sebastiani [Seb02] for a survey of recent
applications of machine learning to text classification.

[DHS01, Wol95]indicate there is no a priori choice of algorithm
which will perform the best over all problems,


[DHS01] Richard Duda, Peter Hart, and David Stork. Pattern Classification. John
Wiley & Sons, Inc., New York, NY, 2001


[DGL96] Luc Devroye, L´aszl´o Gy¨orfi, and G´abor Lugosi. A Probabilistic
Pattern Recognition. Springer-Verlag, New York, NY, 1996.

[Seb02] Fabrizio Sebastiani. Machine learning in automated text categorization. ACM
Computing Surveys, 34(1):1–47, March 2002.


[CD00] Hao Chen and Susan T. Dumais. Bringing order to the web: Automatically
categorizing search results. In CHI ’00, pages 145–152, 2000.

[Yan99] Yiming Yang. An evaluation of statistical approaches to text categorization.
Information Retrieval, 1(1/2):67–88, 1999.

[YL99] Yiming Yang and Xin Liu. A re-examination of text categorization methods.
In SIGIR ’99, Proceedings of the 22nd Annual International ACM Conference
on Research and Development in Information Retrieval, pages 42–49, 1999.

[SS00] Robert E. Schapire and Yoram Singer. BoosTexter: A boosting-based system
for text categorization. Machine Learning, 39:135–168, 2000.



Need for Paralellization:

- Transductive Learning requires to solve the same problem many times!
- There actual trends in Computer Processor are multi-core architetures, allowing and encoraging to


  create parallel solutions to standard problems.


About Machine Learning Framework:

- Writte about what kind of problem solves machine learning 
- Write about empirical riks minimization
-  Write about text representation .


Collobert et Al about transductive SVM
One way of justifying this algorithm, in the context of semi-supervised learning is that one is
finding a decision boundary that lies in a region of low density, implementing the so-called cluster
assumption (see e.g. Chapelle and Zien, 2005). In this framework, if you believe the underlying
distribution of the two classes is such that there is a “gap” or low density region between them,
then TSVMs can help because it selects a rule with exactly those properties. Vapnik (1995) has a
different interpretation for the success of TSVMs, rooted in the idea that transduction (labeling a
test set) is inherently easier than induction (learning a general rule). In either case, experimentally
 c 2006 Ronan Collobert, Fabian Sinz, Jason Weston and L ́ on Bottou.
                                                        e
                             COLLOBERT, SINZ, WESTON AND BOTTOU
it seems clear that algorithms such as TSVMs can give considerable improvement in generalization
over SVMs, if the number of labeled points is small and the number of unlabeled points is large.


About SVM-Light


SVMLight-TSVM Like our work, the heuristic optimization algorithm implemented in SVM-
Light (Joachims, 1999b) solves successive SVM optimization problems, but on L + U instead of
L + 2U data points. It improves the objective function by iteratively switching the labels of two
unlabeled points xi and x j with ξi + ξ j > 2. It uses two nested loops to optimize a TSVM which
solves a quadratic program in each step. The convergence proof of the inner loop relies on the fact
that there is only a finite number 2U of labelings of U unlabeled points, even though it is unlikely
that all of them are examined. However, since the heuristic only swaps the labels of two unlabeled
examples at each step in order to enforce the balancing constraint, it might need many iterations to
reach a minimum, which makes it intractable for big data set sizes in practice (cf. Figure 3).
                                                1693
                             COLLOBERT, SINZ, WESTON AND BOTTOU
    SVMLight uses annealing heuristics for the selection of C∗ . It begins with a small value of C∗
(C∗ = 1e − 5), and multiplies C∗ by 1.5 on each iteration until it reaches C. The numbers 1e − 5 and
1.5 are hard coded into the implementation. On each iteration the tolerance on the gradients is also
changed so as to give more approximate (but faster) solutions on earlier iterations. Again, several
heuristics parameters are hard coded into the implementation.




